# -*- coding: utf-8 -*-
"""06532025_Churning_Customers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wfy2qFvAFnKtqkUv2xnzPQwcdj6ICSwV

# **IMPORT**
"""

pip install scikeras

import keras
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import classification_report
from google.colab import drive
from keras.models import Model
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Input, Dense
from keras.layers import Dense, Input
from keras.utils import to_categorical
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from scikeras.wrappers import KerasClassifier

# Mount Google Drive in Google Colab
from google.colab import drive
drive.mount('/content/drive')

# Import the Pandas library
import pandas as pd

# Specify the path to the CSV file in Google Drive and read it into a DataFrame
churning_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv')

"""
# **EXPLORATORY DATA ANALYSIS**"""

churning_data

# Display the first 5 rows of the churning_data DataFrame
churning_data.head()

# Display information about the DataFrame structure, data types, and memory usage
churning_data.info()

# Display descriptive statistics of numeric columns in the churning_data DataFrame
churning_data.describe()

# Convert the 'TotalCharges' column in the 'churning_data' DataFrame to numeric values
churning_data['TotalCharges'] = pd.to_numeric(churning_data.TotalCharges, errors="coerce")

# Count the number of missing values in each column of the churning_data DataFrame
churning_data.isnull().sum()

# Fill missing values in the 'TotalCharges' column of the 'churning_data' DataFrame to replace NaN values.
churning_data['TotalCharges']=churning_data['TotalCharges'].fillna(method='bfill')

# Display the data types of each column in the churning_data DataFrame
churning_data.dtypes

# Iterate through each column in the churning_data DataFrame
for col in churning_data.columns:
    # Use pd.factorize to encode categorical values with unique integer labels, and assign to the same column
    if churning_data[col].dtype == 'object':
      churning_data[col] = pd.factorize(churning_data[col])[0]

# Create feature variables (X) by copying the churning_data DataFrame and dropping the "Churn" column
X = churning_data.copy().drop("Churn", axis=1)
X = X.drop("customerID", axis=1)

# Create the target variable (y) by selecting the "Churn" column
y = churning_data["Churn"]

scaler = MinMaxScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

scaled_churning_data = pd.concat([X_scaled, y], axis=1)

"""
# **FEATURE SELECTION**"""

# Create a Random Forest Classifier model with 100 decision trees for feature importance
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# rf_model_combined = RandomForestClassifier(n_estimators=100, random_state=42)
rfecv = RFECV(estimator=rf_model, step=1, cv=3, scoring='accuracy')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# Assuming X_train is your feature matrix
selected_features = X_train.columns[rfecv.support_]

optimal_num_features = rfecv.n_features_

# Select the optimal number of features from the dataset
X = X_train[selected_features[:optimal_num_features]]

"""
# **TRAINING**"""

X=scaled_churning_data[selected_features]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def build_classifying_model():
    # Create a Sequential model
    model = Sequential()

    # Define the input layer with the shape of the training data
    input_layer = Input(shape=(X_train.shape[1],))

    # Add the first hidden layer with 64 units and ReLU activation
    hidden_layer_1 = Dense(64, activation='relu')(input_layer)

    # Add the second hidden layer with 32 units and ReLU activation
    hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)

    # Add the third hidden layer with 16 units and ReLU activation
    hidden_layer_3 = Dense(16, activation='relu')(hidden_layer_2)

    # Define the output layer with 1 unit and Sigmoid activation for binary classification
    output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

    # Create a functional Model with defined input and output layers
    model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model with the specified optimizer, binary crossentropy loss, and accuracy metric
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Return the compiled model
    return model

# Create a KerasClassifier using the build_classifying_model function
model = KerasClassifier(build_fn=build_classifying_model)

# Define a K-Fold cross-validation strategy with 3 splits and shuffling
cv = KFold(n_splits=3, shuffle=True)

# Define a dictionary of hyperparameters for grid search
params = {
    'epochs': [10,50,100],
    'optimizer': ['adam', 'adadelta'],
}

# Create a GridSearchCV object with the KerasClassifier model and hyperparameter grid
gs = GridSearchCV(estimator=model, param_grid=params, cv=cv)

# Fit the grid search model to the training data, specifying validation data for monitoring
gs = gs.fit(X_train, y_train, validation_data=(X_test, y_test))

print("Best Parameters:", gs.best_params_)
print("Best Accuracy:", gs.best_score_)

# Assuming best_params contains values for 'batch_size' and 'epochs' as well
best_params = gs.best_params_

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(16, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

# Create the best model based on the hyperparameters from grid search
best_model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model with the best optimizer (assuming 'adam' is the best optimizer)
best_model.compile(optimizer=best_params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using the best hyperparameters
best_model.fit(X_train, y_train, epochs=best_params['epochs'], validation_data=(X_test, y_test))

# Get accuracy
_, accuracy = best_model.evaluate(X_train, y_train)
accuracy*100

# Import the metrics module
from sklearn import metrics

# Evaluate the model and obtain predicted probabilities
y_pred = best_model.predict(X_test)
fpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test, y_pred)
auc_mlp = round(metrics.roc_auc_score(y_test, y_pred), 4)
print("AUC:", auc_mlp)

# Assuming you want to round the predicted values to integers (0 or 1)
y_pred = np.round(best_model.predict(X_test)).ravel()

"""#**OPTIMIZATION**"""

from keras.models import Model
from keras.layers import Input, Dense, Dropout
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV, KFold
from sklearn import metrics
import numpy as np

dropout_rates = [0.5, 0.7]
best_optimized_accuracy = 0.0
best_optimized_params = {}

def build_classifying_model(dropout_rate=0.3, optimizer='adam'):
  input_layer = Input(shape=(X_train.shape[1],))
  hidden_layer_1 = Dense(64, activation='relu')(input_layer)
  hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
  hidden_layer_3 = Dense(16, activation='relu')(hidden_layer_2)
  output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

  model = Model(inputs=input_layer, outputs=output_layer)

  # Compile the model with the given optimizer
  model.compile(optimizer=best_params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])

  # Use best_params for other parameters like batch_size and epochs
  model.fit(X_train, y_train, epochs=best_params['epochs'], validation_data=(X_test, y_test))

  return model

model = KerasClassifier(build_fn=build_classifying_model)
cv = KFold(n_splits=5, shuffle=True)

params = {
    'batch_size':[16,32,64]}

optimized_gs = GridSearchCV(estimator=model, param_grid=params, cv=cv)
optimized_gs.fit(X_train, y_train, validation_data=(X_test, y_test))

# Evaluate the model and obtain accuracy
accuracy = optimized_gs.best_score_

# Check if the current model has a higher accuracy
if accuracy > best_optimized_accuracy:
    best_optimized_accuracy = accuracy
    best_optimized_params = optimized_gs.best_params_

# Assuming best_params contains values for 'batch_size' and 'epochs' as well
best_optimized_params = optimized_gs.best_params_
best_optimized_params

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(16, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

best_optimized_model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model with the best optimizer (assuming 'adam' is the best optimizer)
best_optimized_model.compile(optimizer=best_params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])

# Use best_params for other parameters like batch_size and epochs
best_optimized_model.fit(X_train, y_train, batch_size=best_optimized_params['batch_size'], epochs=best_params['epochs'], validation_data=(X_test, y_test))

# Get accuracy
_, accuracy = best_optimized_model.evaluate(X_train, y_train)
accuracy*100

"""# **TESTING**"""

loss, accuracy = best_optimized_model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

# Evaluate the model and obtain predicted probabilities
y_pred = best_optimized_model.predict(X_test)
fpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test, y_pred)
auc_mlp = round(metrics.roc_auc_score(y_test, y_pred), 4)
print("AUC:", auc_mlp)

# Assuming you want to round the predicted values to integers (0 or 1)
y_pred = np.round(best_optimized_model.predict(X_test)).ravel()

"""
# **DEPLOYMENT**"""

import joblib

# With 'best_optimized_model' is your trained model
model_path = '/content/drive/My Drive/Colab Notebooks/best_opimized.model.joblib'

# Save the model to the specified file path
joblib.dump(best_optimized_model, model_path)